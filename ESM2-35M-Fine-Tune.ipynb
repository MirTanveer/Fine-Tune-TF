{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa7262-b504-40bf-944d-5d1a62f0bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, random\n",
    "\n",
    "seed = 40\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da9211-e4af-4ad4-9bb8-127f0b0faf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9055e-33e6-4766-94d9-c81359bd26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "local_fasta_path = 'TF_Training_Labeled_Combined.fasta'\n",
    "\n",
    "# Load FASTA file using Biopython\n",
    "sequences = []\n",
    "for record in SeqIO.parse(local_fasta_path, \"fasta\"):\n",
    "    # Split the description to extract label\n",
    "    description_parts = record.description.split(\"%\")\n",
    "    label = int(description_parts[-1].split(\"LABEL=\")[1])  # Extracting the numeric part of the label\n",
    "    sequences.append([record.name, str(record.seq), label])\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(sequences, columns=[\"name\", \"sequence\", \"label\"])\n",
    "\n",
    "# Display the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4460e-e7bd-474d-a892-6e03a370439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert label column to int (already done)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Optional: Map label to class names if needed\n",
    "label2id = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Convert dataframe to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abe243-ae76-4154-a1cb-1c1c8b3a3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed87d47-8587-4e4d-b600-3d517f809b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomHeadClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, dropout, num_classes):\n",
    "        super(CustomHeadClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x shape: (batch_size, seq_len, embedding_size)\n",
    "        if attention_mask is not None:\n",
    "            # masked mean pooling\n",
    "            mask = attention_mask.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "            x = (x * mask).sum(1) / mask.sum(1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)  # mean pooling over sequence\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22405ee7-c2ab-487a-a074-2eb24b6a8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenzier and model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"facebook/esm2_t12_35M_UR50D\"  # or other ESM-2 variant\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "\n",
    "embedding_size = model.config.hidden_size\n",
    "model.classifier = CustomHeadClassifier(embedding_size, hidden_size=128, dropout=0.3, num_classes=2)\n",
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4586adb-d725-487d-9deb-f59a102c3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze classification head\n",
    "# for param in model.classifier.parameters():  \n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbbc7a-9a12-4f1f-9cd2-6633411b8bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93c95d-784a-4696-9f75-6a89eab5bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {all_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5ba34-5f51-4250-9e35-1c1e4732246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sequence\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a709b74-70ac-4c25-b0a6-06272e9475c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "frozen = [name for name, param in model.named_parameters() if not param.requires_grad]\n",
    "\n",
    "print(f\"Trainable layers: {len(trainable)}\")\n",
    "print(trainable[:5])  # print a few\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Frozen layers: {len(frozen)}\")\n",
    "print(frozen[:5])  # print a few\n",
    "\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ESM2-35M-with-Finetune\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2, #Effective value should be 8\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81e0b9-0e34-4c75-9596-7716f6352e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958d2e3-35c5-4c9d-ac2b-e7358febd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c06f-b819-4188-9dd6-9f3a13667f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88011f9f-0d38-41c5-bddf-f018ad6f76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data; replace these with your real log-derived lists\n",
    "#epochs = [1, 2, 3, 4, 5]\n",
    "# train_loss = [0.5202, 0.309, 0.2708, 0.2419, 0.2284]\n",
    "# eval_loss = [0.4150600731372833, 0.3777799904346466,0.43656858801841736,0.36971646547317505,0.3870314955711365]\n",
    "# eval_accuracy = [0.8383233532934131,0.8622754491017964,0.874251497005988,0.8922155688622755,0.8922155688622755]\n",
    "\n",
    "\n",
    "train_loss, eval_loss, eval_accuracy, epochs = [], [], [], []\n",
    "\n",
    "for entry in trainer.state.log_history:\n",
    "    if \"loss\" in entry and \"epoch\" in entry:\n",
    "        train_loss.append(entry[\"loss\"])\n",
    "    if \"eval_loss\" in entry:\n",
    "        eval_loss.append(entry[\"eval_loss\"])\n",
    "    if \"eval_accuracy\" in entry:\n",
    "        eval_accuracy.append(entry[\"eval_accuracy\"])\n",
    "    if \"epoch\" in entry:\n",
    "        epochs.append(entry[\"epoch\"])\n",
    "\n",
    "# Make sure x-axis aligns\n",
    "x = list(range(1, len(train_loss)+1))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "# Plot loss curves on the left y-axis\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(x, train_loss,   marker='o')\n",
    "ax1.plot(x, eval_loss,   marker='s')\n",
    "ax1.set_xticks(x)\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# Instantiate a second axes sharing the same x-axis for accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.plot(x, eval_accuracy, color='green', marker='^')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Combine legends\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "#ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper center')\n",
    "ax2.set_ylim(0.8, 0.92)\n",
    "plt.title('Fine tuning ESM2 35M', fontsize=11)\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Figures/With_FineTuning/Fine_tuning_ESM2_35M_without_LoRA.png', dpi=400)  # Save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856bd4a-2d92-462a-880a-6d28981be374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save\n",
    "# Save model, tokenzier, and Custom Head Classifier separately\n",
    "# The standard save_pretrained method from Transformers won’t automatically save the custom head class code. \n",
    "# We need to save both the model weights and the custom head properly.\n",
    "\n",
    "save_dir = \"Saved_Models/fine_tuned_ESM2_35M_model_with_custom_head\"\n",
    "\n",
    "# Save Hugging Face model weights (excluding custom head)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Save custom head weights separately\n",
    "torch.save(model.classifier.state_dict(), f\"{save_dir}/custom_head.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cb0c6-023c-4f82-8e13-59435988887f",
   "metadata": {},
   "source": [
    "## Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba527b80-460b-4416-8152-52a13b048851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Saved Model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomHeadClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, dropout, num_classes):\n",
    "        super(CustomHeadClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1)\n",
    "            x = (x * mask).sum(1) / mask.sum(1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfa991-d5d8-41e5-8449-92b271f5695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"Saved_Models/fine_tuned_ESM2_35M_model_with_custom_head\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model_load = AutoModelForSequenceClassification.from_pretrained(save_dir, num_labels=2)\n",
    "embedding_size = model_load.config.hidden_size\n",
    "model_load.classifier = CustomHeadClassifier(embedding_size, hidden_size=128, dropout=0.3, num_classes=2)\n",
    "\n",
    "# Load the trained weights\n",
    "model_load.classifier.load_state_dict(torch.load(f\"{save_dir}/custom_head.pt\"))\n",
    "\n",
    "# Move to GPU if needed\n",
    "model_laod = model_load.to(\"cuda:0\")\n",
    "model_load.eval()  # set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad991b-c416-4a9f-a8ee-d848c9699a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e55ca-a1f7-47ff-b147-197d3e888282",
   "metadata": {},
   "source": [
    "## Test on the Independent Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfec9b-837c-437c-8a5b-7b70572737fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "local_fasta_path = 'TF_Ind_Labeled_Combined.txt'\n",
    "\n",
    "# Load FASTA file using Biopython\n",
    "sequences = []\n",
    "for record in SeqIO.parse(local_fasta_path, \"fasta\"):\n",
    "    # Split the description to extract label\n",
    "    description_parts = record.description.split(\"%\")\n",
    "    label = int(description_parts[-1].split(\"LABEL=\")[1])  # Extracting the numeric part of the label\n",
    "    sequences.append([record.name, str(record.seq), label])\n",
    "\n",
    "# Create dataframe\n",
    "df_test = pd.DataFrame(sequences, columns=[\"name\", \"sequence\", \"label\"])\n",
    "\n",
    "# Display the dataframe\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8c6b7-19a4-422f-86c4-fedaea333bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9a4b3-19f9-42c2-bbb4-477ce3f9744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(int)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35a18d-f647-4fac-8520-a9d32731944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sequence\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f169ee-3a5a-4c11-8921-423c19ae06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model_load, tokenizer=tokenizer)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed4fa6-c9e6-41b1-8012-4bbd22e062e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- Extract predictions ---\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "probs = F.softmax(torch.tensor(logits), dim=-1).numpy()  # shape: [N, num_classes]\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Define function to compute metrics ---\n",
    "def compute_metrics(labels, preds, probs):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    numerator = (TP * TN) - (FP * FN)\n",
    "    denominator = np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    mcc = numerator / denominator if denominator > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, probs[:, 1]) if probs.shape[1] > 1 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": precision,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Recall\": recall,\n",
    "        \"MCC\": mcc,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "    }\n",
    "\n",
    "# # --- Compute metrics on full dataset ---\n",
    "# metrics = compute_metrics(labels, preds, probs)\n",
    "# print(\"=== Metrics on Full Data ===\")\n",
    "# for k, v in metrics.items():\n",
    "#     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(labels, preds))\n",
    "\n",
    "# --- Bootstrapping for Std Dev ---\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(42)\n",
    "boot_metrics = {k: [] for k in metrics.keys()}\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(labels), len(labels))  # sample with replacement\n",
    "    boot_labels = labels[indices]\n",
    "    boot_preds = preds[indices]\n",
    "    boot_probs = probs[indices]\n",
    "\n",
    "    m = compute_metrics(boot_labels, boot_preds, boot_probs)\n",
    "    for k in m:\n",
    "        boot_metrics[k].append(m[k])\n",
    "\n",
    "# --- Compute mean and std ---\n",
    "print(\"\\n=== Bootstrapped Metrics (Mean ± Std) ===\")\n",
    "for k in metrics.keys():\n",
    "    mean_val = np.mean(boot_metrics[k])\n",
    "    std_val = np.std(boot_metrics[k])\n",
    "    print(f\"{k}: {mean_val:.4f} ± {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03a142-ff43-4e2d-933e-edda9394d258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
