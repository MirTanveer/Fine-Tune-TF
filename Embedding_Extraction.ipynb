{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06079823-7269-4e98-bf48-7a278baba1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "from Bio import SeqIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341f98e-27d6-499b-8a00-b5a6faa32baa",
   "metadata": {},
   "source": [
    "## ESM2 variant embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965fd5e-d3fe-441e-a757-ffccd42af04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "\n",
    "# Define models and FASTA files\n",
    "\n",
    "models = {\n",
    "    \"esm2_t6_8M_UR50D\": 6,\n",
    "    \"esm2_t12_35M_UR50D\": 12,\n",
    "    \"esm2_t30_150M_UR50D\": 30,\n",
    "    \"esm2_t33_650M_UR50D\": 33,\n",
    "    \"esm2_t36_3B_UR50D\": 36,\n",
    "}\n",
    "\n",
    "fasta_files = [\n",
    "    \"TF_Training.txt\",\n",
    "    \"NTF_training.txt\",\n",
    "    \"TF_Ind.txt\",\n",
    "    \"NTF_Ind.txt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853f4c4-6052-470c-bbaa-8cef3aec268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function to generate embeddings\n",
    "\n",
    "def generate_embeddings(model_name, layer, fasta_file):\n",
    "    print(f\"\\nðŸ”¹ Processing {fasta_file} with {model_name} ...\")\n",
    "\n",
    "    # Load model\n",
    "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()\n",
    "\n",
    "    # Read sequences\n",
    "    sequences = [(record.id, str(record.seq)) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "    #mean_embeddings = {}\n",
    "    cls_embeddings = {}\n",
    "\n",
    "    batch_size = 8\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch_seqs = sequences[i : i + batch_size]\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                batch_tokens = batch_tokens.cuda()\n",
    "                model = model.cuda()\n",
    "\n",
    "            # Extract embeddings\n",
    "            results = model(batch_tokens, repr_layers=[layer])\n",
    "            token_embeddings = results[\"representations\"][layer]\n",
    "\n",
    "            # Save both mean-pooled and CLS\n",
    "            for j, (seq_id, seq) in enumerate(batch_seqs):\n",
    "                seq_len = (batch_tokens[j] != alphabet.padding_idx).sum()\n",
    "\n",
    "                cls_emb = token_embeddings[j, 0].cpu()  # CLS\n",
    "                mean_emb = token_embeddings[j, 1:seq_len-1].mean(0).cpu()  # mean over residues\n",
    "\n",
    "                mean_embeddings[seq_id] = mean_emb\n",
    "                cls_embeddings[seq_id] = cls_emb\n",
    "\n",
    "    # Save files separately\n",
    "    base_name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "\n",
    "    mean_file = f\"{model_name}_{base_name}_mean.pt\"\n",
    "    cls_file = f\"{model_name}_{base_name}_cls.pt\"\n",
    "\n",
    "    torch.save(mean_embeddings, mean_file)\n",
    "    torch.save(cls_embeddings, cls_file)\n",
    "\n",
    "    \n",
    "    # Verify saved files\n",
    "    \n",
    "    loaded_mean = torch.load(mean_file)\n",
    "    loaded_cls = torch.load(cls_file)\n",
    "    first_id = list(loaded_mean.keys())[0]\n",
    "\n",
    "    print(f\"âœ… Saved {len(loaded_mean)} mean embeddings to {mean_file} (shape: {loaded_mean[first_id].shape})\")\n",
    "    print(f\"âœ… Saved {len(loaded_cls)} CLS embeddings to {cls_file} (shape: {loaded_cls[first_id].shape})\")\n",
    "\n",
    "\n",
    "\n",
    "# Run for all models and FASTAs\n",
    "\n",
    "for model_name, layer in models.items():\n",
    "    for fasta_file in fasta_files:\n",
    "        generate_embeddings(model_name, layer, fasta_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bd3b2-ced2-4f4a-a7ac-279b9fb1be0f",
   "metadata": {},
   "source": [
    "## Ankh-base model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedced0-ae44-4036-955f-3222f86b6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\n",
    "# from Bio import SeqIO\n",
    "\n",
    "# \n",
    "# # Config\n",
    "# \n",
    "# fasta_files = [\"TF_training.txt\", \"NTF_training.txt\", \"TF_ind.txt\", \"NTF_ind.txt\"]\n",
    "\n",
    "# models = {\n",
    "#     \"protbert\": {\n",
    "#         \"name\": \"Rostlab/prot_bert\",\n",
    "#         \"tokenizer\": lambda: BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False),\n",
    "#         \"model\": lambda: BertModel.from_pretrained(\"Rostlab/prot_bert\"),\n",
    "#         \"hidden_size\": 1024,\n",
    "#     },\n",
    "#     \"ankh_base\": {\n",
    "#         \"name\": \"ankh-models/ankh-base\",\n",
    "#         \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\"ElnaggarLab/ankh-base\"),\n",
    "#         \"model\": lambda: AutoModel.from_pretrained(\"ElnaggarLab/ankh-base\"),\n",
    "#         \"hidden_size\": 768,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# \n",
    "# # Embedding extraction function\n",
    "# \n",
    "# def extract_embeddings(model_key, fasta_file):\n",
    "#     print(f\"â–¶ Processing {model_key} on {fasta_file}\")\n",
    "\n",
    "#     tokenizer = models[model_key][\"tokenizer\"]()\n",
    "#     model = models[model_key][\"model\"]()\n",
    "#     model.eval()\n",
    "#     if torch.cuda.is_available():\n",
    "#         model = model.cuda()\n",
    "\n",
    "#     sequences = [(record.id, str(record.seq)) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "#     mean_embeddings = {}\n",
    "#     #cls_embeddings = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for seq_id, seq in sequences:\n",
    "#             seq_spaced = \" \".join(list(seq))  # space-separate amino acids\n",
    "#             inputs = tokenizer(seq_spaced, return_tensors=\"pt\", add_special_tokens=True)\n",
    "#             if torch.cuda.is_available():\n",
    "#                 inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "#             outputs = model(**inputs)\n",
    "#             last_hidden = outputs.last_hidden_state  # (1, L, hidden_size)\n",
    "\n",
    "#             # [CLS] embedding = first token\n",
    "#             #cls_emb = last_hidden[:, 0, :].squeeze().cpu()\n",
    "#             # Mean embedding (excluding CLS + SEP)\n",
    "#             mean_emb = last_hidden[:, 1:-1, :].mean(1).squeeze().cpu()\n",
    "\n",
    "#             #cls_embeddings[seq_id] = cls_emb\n",
    "#             mean_embeddings[seq_id] = mean_emb\n",
    "\n",
    "#     # Save\n",
    "#     base_name = fasta_file.replace(\".txt\", \"\")\n",
    "#     torch.save(mean_embeddings, f\"{model_key}_{base_name}_mean.pt\")\n",
    "#     #torch.save(cls_embeddings, f\"{model_key}_{base_name}_cls.pt\")\n",
    "\n",
    "#     print(f\"âœ… Saved: {model_key}_{base_name}_mean.pt\")\n",
    "\n",
    "\n",
    "# \n",
    "# # Run extraction\n",
    "# \n",
    "# for model_key in models.keys():\n",
    "#     for fasta_file in fasta_files:\n",
    "#         extract_embeddings(model_key, fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a2d0d-acd0-4f3e-86b8-babb0b6574f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "# Config\n",
    "\n",
    "fasta_files = [\"TF_training.txt\", \"NTF_training.txt\", \"TF_ind.txt\", \"NTF_ind.txt\"]\n",
    "\n",
    "model_name = \"ElnaggarLab/ankh-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "encoder = model.encoder  # use encoder only (avoid decoder requirement)\n",
    "encoder.eval()\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "\n",
    "\n",
    "# Embedding extraction function\n",
    "\n",
    "def extract_embeddings(fasta_file):\n",
    "    print(f\"â–¶ Processing {fasta_file}\")\n",
    "\n",
    "    sequences = [(record.id, str(record.seq)) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "    mean_embeddings = {}\n",
    "    #cls_embeddings = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq_id, seq in sequences:\n",
    "            seq_spaced = \" \".join(list(seq))  # space-separate amino acids\n",
    "            inputs = tokenizer(seq_spaced, return_tensors=\"pt\", add_special_tokens=True)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "            outputs = encoder(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state  # (1, L, hidden_size)\n",
    "\n",
    "            # [CLS] embedding = first token\n",
    "            cls_emb = last_hidden[:, 0, :].squeeze().cpu()\n",
    "            # Mean embedding (excluding CLS + SEP)\n",
    "            mean_emb = last_hidden[:, 1:-1, :].mean(1).squeeze().cpu()\n",
    "\n",
    "            cls_embeddings[seq_id] = cls_emb\n",
    "            mean_embeddings[seq_id] = mean_emb\n",
    "\n",
    "    # Save\n",
    "    base_name = fasta_file.replace(\".txt\", \"\")\n",
    "    torch.save(mean_embeddings, f\"ankh_base_{base_name}_mean.pt\")\n",
    "    torch.save(cls_embeddings, f\"ankh_base_{base_name}_cls.pt\")\n",
    "\n",
    "    print(f\"âœ… Saved: ankh_base_{base_name}_mean.pt, ankh_base_{base_name}_cls.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Run for all fasta files\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    extract_embeddings(fasta_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0b229-c7b9-4135-9c19-9fa4e6b03dd2",
   "metadata": {},
   "source": [
    "## Protbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54019e-d9ce-4f66-af40-bccb5ce4289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "# Config\n",
    "\n",
    "fasta_files = [\"TF_training.txt\", \"NTF_training.txt\", \"TF_ind.txt\", \"NTF_ind.txt\"]\n",
    "\n",
    "# ProtBert model\n",
    "model_name = \"Rostlab/prot_bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "# Embedding extraction function\n",
    "\n",
    "def extract_embeddings(fasta_file):\n",
    "    print(f\"â–¶ Processing {fasta_file}\")\n",
    "\n",
    "    sequences = [(record.id, str(record.seq)) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "    mean_embeddings = {}\n",
    "    cls_embeddings = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq_id, seq in sequences:\n",
    "            seq_spaced = \" \".join(list(seq))  # ProtBert expects space-separated AAs\n",
    "            inputs = tokenizer(seq_spaced, return_tensors=\"pt\", add_special_tokens=True)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state  # (1, L, hidden_size)\n",
    "\n",
    "            # [CLS] embedding = first token\n",
    "            cls_emb = last_hidden[:, 0, :].squeeze().cpu()\n",
    "            # Mean embedding (excluding CLS + SEP)\n",
    "            mean_emb = last_hidden[:, 1:-1, :].mean(1).squeeze().cpu()\n",
    "\n",
    "            cls_embeddings[seq_id] = cls_emb\n",
    "            mean_embeddings[seq_id] = mean_emb\n",
    "\n",
    "    # Save\n",
    "    base_name = fasta_file.replace(\".txt\", \"\")\n",
    "    torch.save(mean_embeddings, f\"protbert_{base_name}_mean.pt\")\n",
    "    torch.save(cls_embeddings, f\"protbert_{base_name}_cls.pt\")\n",
    "\n",
    "    print(f\"âœ… Saved: protbert_{base_name}_mean.pt, protbert_{base_name}_cls.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Run for all fasta files\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    extract_embeddings(fasta_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
