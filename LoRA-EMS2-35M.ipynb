{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c84d8-99a6-42af-9c09-eb26d9359d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04b7f9-64ad-4cdb-bac8-1f27d9657b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training data (TF vs NTF)\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "local_fasta_path = 'TF_Training_Labeled_Combined.fasta'\n",
    "\n",
    "# Load FASTA file using Biopython\n",
    "sequences = []\n",
    "for record in SeqIO.parse(local_fasta_path, \"fasta\"):\n",
    "    # Split the description to extract label\n",
    "    description_parts = record.description.split(\"%\")\n",
    "    label = int(description_parts[-1].split(\"LABEL=\")[1])  # Extracting the numeric part of the label\n",
    "    sequences.append([record.name, str(record.seq), label])\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(sequences, columns=[\"name\", \"sequence\", \"label\"])\n",
    "\n",
    "# Display the dataframe\n",
    "df.head(5)\n",
    "\n",
    "from datasets import Dataset\n",
    "# Convert label column to int (already done)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Optional: Map label to class names if needed\n",
    "label2id = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Convert dataframe to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e17d5-15cc-4566-864b-fffa2c523f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e2a4c-e8be-493f-bfd8-7d80d95ffffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Custom Head Classifier\n",
    "class CustomHeadClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, dropout, num_classes):\n",
    "        super(CustomHeadClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x shape: (batch_size, seq_len, embedding_size)\n",
    "        if attention_mask is not None:\n",
    "            # masked mean pooling\n",
    "            mask = attention_mask.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "            x = (x * mask).sum(1) / mask.sum(1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)  # mean pooling over sequence\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d69ef-acec-4220-9d1d-589323b62c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load base model (e.g., ESM-2 or ProtBERT etc.)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t12_35M_UR50D\", num_labels=2)\n",
    "\n",
    "embedding_size = base_model.config.hidden_size\n",
    "base_model.classifier = CustomHeadClassifier(embedding_size, hidden_size=128, dropout=0.3, num_classes=2)\n",
    "\n",
    "#Freeze base model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\", \"dense\"],\n",
    "    bias=\"all\",\n",
    "    task_type=TaskType.SEQ_CLS)\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Restrict trainable LoRA adapters to encoder blocks 23–24\n",
    "# def is_target_block(name):\n",
    "#     return any(f\"transformer.encoder.block.{i}.\" in name for i in range(22, 23)) and \"lora_\" in name\n",
    "\n",
    "# def is_target_block(name):\n",
    "#     return (\n",
    "#         any(f\"esm.encoder.layer.{i}.\" in name for i in range(1, 10))  # inclusive of 22 and 23\n",
    "#         and \"query.lora_A\" in name\n",
    "#         and name.endswith(\".weight\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526e0c4-1dbd-4965-9683-267bcd82981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c55eba-5c81-4dcf-b7b4-3c6d7ca7fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     param.requires_grad = is_target_block(name)\n",
    "\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86482be-1a05-485d-aefb-3fef1dab7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db24783-d15e-429b-adc6-8435bb7019b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab421e-9420-42e6-9baf-f7f0074e5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_modules = [\n",
    "#     \"transformer.encoder.block.19.layer.0.SelfAttention.q\",\n",
    "#     \"transformer.encoder.block.19.layer.0.SelfAttention.v\",\n",
    "#     \"transformer.encoder.block.20.layer.0.SelfAttention.q\",\n",
    "#     \"transformer.encoder.block.20.layer.0.SelfAttention.v\",\n",
    "#     \"transformer.encoder.block.21.layer.0.SelfAttention.q\",\n",
    "#     \"transformer.encoder.block.21.layer.0.SelfAttention.v\",\n",
    "#     \"transformer.encoder.block.22.layer.0.SelfAttention.q\",\n",
    "#     \"transformer.encoder.block.22.layer.0.SelfAttention.v\",\n",
    "#     \"transformer.encoder.block.23.layer.0.SelfAttention.q\",\n",
    "#     \"transformer.encoder.block.23.layer.0.SelfAttention.v\"\n",
    "# ]\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_CLS,\n",
    "#     r=4,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=target_modules  # << This is key!\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# # Optional: confirm\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe68f61-a727-43b0-8c79-83a475b0e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in base_model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28d316-70c5-4161-8878-d951cb148cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "frozen = [name for name, param in model.named_parameters() if not param.requires_grad]\n",
    "\n",
    "print(f\"Trainable layers: {len(trainable)}\")\n",
    "print(trainable[:5])  # print a few\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Frozen layers: {len(frozen)}\")\n",
    "print(frozen[:5])  # print a few\n",
    "\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./esm2_t12_35M_lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0339637-6c06-42c8-afda-ad1499295e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5Tokenizer\n",
    "tokenizer =AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sequence\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0afc9-5af3-47c4-986d-e9eeec39c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba99f73-0112-4409-b172-ce8b987df242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41a381-4d99-4b4d-9804-c6e23e3acfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract Logs\n",
    "# train_loss = []\n",
    "# eval_loss = []\n",
    "# eval_accuracy = []\n",
    "# train_steps = []\n",
    "# eval_steps = []\n",
    "\n",
    "\n",
    "# for log in trainer.state.log_history:\n",
    "#     if 'loss' in log:\n",
    "#         train_loss.append(log['loss'])\n",
    "#         train_steps.append(log['step'])\n",
    "#     if 'eval_loss' in log:\n",
    "#         eval_loss.append(log['eval_loss'])\n",
    "#         eval_accuracy.append(log['eval_accuracy'])\n",
    "#         eval_steps.append(log['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02454582-7a58-4f15-a5b9-0ce9bb1477cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss, eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e4ac8-96e5-4539-8f12-8b813e2dc34f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data; replace these with your real log-derived lists\n",
    "#epochs = [1, 2, 3, 4, 5]\n",
    "# train_loss = [0.5202, 0.309, 0.2708, 0.2419, 0.2284]\n",
    "# eval_loss = [0.4150600731372833, 0.3777799904346466,0.43656858801841736,0.36971646547317505,0.3870314955711365]\n",
    "# eval_accuracy = [0.8383233532934131,0.8622754491017964,0.874251497005988,0.8922155688622755,0.8922155688622755]\n",
    "\n",
    "\n",
    "train_loss, eval_loss, eval_accuracy, epochs = [], [], [], []\n",
    "\n",
    "for entry in trainer.state.log_history:\n",
    "    if \"loss\" in entry and \"epoch\" in entry:\n",
    "        train_loss.append(entry[\"loss\"])\n",
    "    if \"eval_loss\" in entry:\n",
    "        eval_loss.append(entry[\"eval_loss\"])\n",
    "    if \"eval_accuracy\" in entry:\n",
    "        eval_accuracy.append(entry[\"eval_accuracy\"])\n",
    "    if \"epoch\" in entry:\n",
    "        epochs.append(entry[\"epoch\"])\n",
    "\n",
    "# Make sure x-axis aligns\n",
    "x = list(range(1, len(train_loss)+1))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "# Plot loss curves on the left y-axis\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(x, train_loss,   marker='o')\n",
    "ax1.plot(x, eval_loss,   marker='s')\n",
    "ax1.set_xticks(x)\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# Instantiate a second axes sharing the same x-axis for accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.plot(x, eval_accuracy, color='green', marker='^')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Combine legends\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "#ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper center')\n",
    "#ax2.set_ylim(0.8, 0.95)\n",
    "plt.title('LoRA ESM2 35M', fontsize=11)\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Figures/With_LoRA/New/LoRA_ESM2_35M.png', dpi=400)  # Save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cc1cf-2df0-4b2b-a2d9-d5c6fe3d3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save\n",
    "# Save model, tokenzier, and Custom Head Classifier separately\n",
    "# The standard save_pretrained method from Transformers won’t automatically save the custom head class code. \n",
    "# We need to save both the model weights and the custom head properly.\n",
    "\n",
    "save_dir = \"Saved_Models/LoRA_ESM2_35M_model_with_custom_head\"\n",
    "\n",
    "# Save Hugging Face model weights (excluding custom head)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Save custom head weights separately\n",
    "torch.save(model.classifier.state_dict(), f\"{save_dir}/custom_head.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0046137-9568-4a15-b56a-8ad9d7332040",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b95ec7-3faa-4c9f-a3f7-529757dff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fasta_path = 'TF_Ind_Labeled_Combined.txt'\n",
    "\n",
    "# Load FASTA file using Biopython\n",
    "sequences = []\n",
    "for record in SeqIO.parse(local_fasta_path, \"fasta\"):\n",
    "    # Split the description to extract label\n",
    "    description_parts = record.description.split(\"%\")\n",
    "    label = int(description_parts[-1].split(\"LABEL=\")[1])  # Extracting the numeric part of the label\n",
    "    sequences.append([record.name, str(record.seq), label])\n",
    "\n",
    "# Create dataframe\n",
    "df_test = pd.DataFrame(sequences, columns=[\"name\", \"sequence\", \"label\"])\n",
    "\n",
    "# Display the dataframe\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec9dcb-144c-4861-8989-a79e1fb2d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(int)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b8b55-94e0-4f59-b2c8-e349e0ab89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee02e3b-9005-4b81-bc27-97632fa5487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer_test = Trainer(model=model, tokenizer=tokenizer)\n",
    "\n",
    "predictions = trainer_test.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9a0ea-c1f6-44d9-b47c-7f402c1d3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- Extract predictions ---\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "probs = F.softmax(torch.tensor(logits), dim=-1).numpy()  # shape: [N, num_classes]\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Define function to compute metrics ---\n",
    "def compute_metrics(labels, preds, probs):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    numerator = (TP * TN) - (FP * FN)\n",
    "    denominator = np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    mcc = numerator / denominator if denominator > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, probs[:, 1]) if probs.shape[1] > 1 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": precision,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Recall\": recall,\n",
    "        \"MCC\": mcc,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "    }\n",
    "\n",
    "# --- Compute metrics on full dataset ---\n",
    "metrics = compute_metrics(labels, preds, probs)\n",
    "print(\"=== Metrics on Full Data ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "# --- Bootstrapping for Std Dev ---\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(42)\n",
    "boot_metrics = {k: [] for k in metrics.keys()}\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(labels), len(labels))  # sample with replacement\n",
    "    boot_labels = labels[indices]\n",
    "    boot_preds = preds[indices]\n",
    "    boot_probs = probs[indices]\n",
    "\n",
    "    m = compute_metrics(boot_labels, boot_preds, boot_probs)\n",
    "    for k in m:\n",
    "        boot_metrics[k].append(m[k])\n",
    "\n",
    "# --- Compute mean and std ---\n",
    "print(\"\\n=== Bootstrapped Metrics (Mean ± Std) ===\")\n",
    "for k in metrics.keys():\n",
    "    mean_val = np.mean(boot_metrics[k])\n",
    "    std_val = np.std(boot_metrics[k])\n",
    "    print(f\"{k}: {mean_val:.4f} ± {std_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
